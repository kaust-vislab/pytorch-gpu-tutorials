{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off of [What is torch.nn really?](https://pytorch.org/tutorials/beginner/nn_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import model_selection, preprocessing\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the MNIST data\n",
    "\n",
    "## Download the data from Kaggle...\n",
    "\n",
    "### 1. Register for an account with Kaggle\n",
    "\n",
    "In order to download Kaggle competition data you will first need to create a [Kaggle](https://www.kaggle.com/) account.\n",
    "\n",
    "### 2. Create an API key\n",
    "\n",
    "Once you have registered for a Kaggle account you will need to create some [API credentials](https://github.com/Kaggle/kaggle-api#api-credentials) in order to be able to use the `kaggle` CLI to download data.\n",
    "\n",
    "### 3. Download the Data\n",
    "\n",
    "Execute the code in the following cell to download the Kaggle [Digit Recognizer: Learn computer vision with the famous MNIST data](https://www.kaggle.com/c/digit-recognizer) competition data. In order for the following Kaggle API call to work you will need to login to your Kaggle account and accept the rules for this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download \\\n",
    "    -c digit-recognizer \\\n",
    "    -p ../data/raw/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...or not!\n",
    "\n",
    "If you don't want to set up an account with Kaggle, then no worries! I have included the training and testing data sets for you.\n",
    "\n",
    "    ../data/raw/mnist/train.csv\n",
    "    ../data/raw/mnist/test/csv\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../data/raw/mnist/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_arr = np.loadtxt(\"../data/raw/mnist/train.csv\", delimiter=',', skiprows=1, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw features are between 0 and 255\n",
    "mnist_arr.min(), mnist_arr.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the MNIST data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prng = np.random.RandomState(42)\n",
    "training_arr, validation_arr = model_selection.train_test_split(mnist_arr, test_size=0.20, random_state=prng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_target, training_features = training_arr[:, 0], training_arr[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_target, validation_features = validation_arr[:, 0], validation_arr[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to rescale the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "scaled_training_features = min_max_scaler.fit_transform(training_features)\n",
    "scaled_validation_features = min_max_scaler.fit_transform(validation_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out a training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,1)\n",
    "_ = ax.imshow(scaled_training_features[0].reshape((28, 28)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses `torch.tensor` rather than `numpy.ndarray` so we need to convert data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_target = torch.tensor(training_target)\n",
    "scaled_training_features = torch.tensor(scaled_training_features, dtype=torch.float32)\n",
    "\n",
    "validation_target = torch.tensor(validation_target)\n",
    "scaled_validation_features = torch.tensor(scaled_validation_features, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_training_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_training_features.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_training_features.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_samples, number_features = scaled_training_features.shape\n",
    "\n",
    "# using Xavier initialization (divide weights by sqrt(number_features))\n",
    "weights = torch.randn(number_features, 10) / math.sqrt(number_features)\n",
    "weights.requires_grad_() # trailing underscore indicates in-place operation\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _linear_transformation(X):\n",
    "    return X @ weights + bias\n",
    "\n",
    "def _log_softmax_activation(X):\n",
    "    return X - X.exp().sum(-1).log().unsqueeze(-1)\n",
    "    \n",
    "def model(X):\n",
    "    Z = _linear_transformation(X)\n",
    "    return _log_softmax_activation(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "output = model(scaled_training_features[:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(output, target):\n",
    "    m, _ = output.shape\n",
    "    return -output[range(m), target].mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_log_likelihood(output, training_target[:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    predictions = torch.argmax(output, dim=1)\n",
    "    return (predictions == target).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(output, training_target[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_epochs = 2\n",
    "number_batches = (number_samples - 1) // batch_size + 1\n",
    "\n",
    "learning_rate = 0.5\n",
    "for epoch in range(number_epochs):\n",
    "    for batch in range(number_batches):\n",
    "        # forward pass\n",
    "        start = batch * batch_size\n",
    "        X = scaled_training_features[start:(start + batch_size)]\n",
    "        y = training_target[start:(start + batch_size)]\n",
    "        output = model(X)\n",
    "        loss = negative_log_likelihood(output, y)\n",
    "        \n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= learning_rate * weights.grad\n",
    "            bias -= learning_rate * bias.grad\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_log_likelihood(model(X), y), accuracy(model(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `torch.nn.functional`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X):\n",
    "    return X @ weights + bias\n",
    "\n",
    "loss_function = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(model(X), y), accuracy(model(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `torch.nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class MNISTLogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self._bias = nn.Parameter(torch.zeros(10))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return X @ self._weights + self._bias\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(model(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, loss_function, number_samples, number_epochs=2, batch_size=64):\n",
    "    \n",
    "    number_batches = (number_samples - 1) // batch_size + 1\n",
    "    for epoch in range(number_epochs):\n",
    "        for batch in range(number_batches):\n",
    "            # forward pass\n",
    "            start = batch * batch_size\n",
    "            X = scaled_training_features[start:(start + batch_size)]\n",
    "            y = training_target[start:(start + batch_size)]\n",
    "            output = model(X)\n",
    "            loss = loss_function(output, y)\n",
    "\n",
    "            # back propagation\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for parameter in model.parameters():\n",
    "                    parameter -= learning_rate * parameter.grad\n",
    "                model.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, loss_function, number_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(model(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring using `nn.Linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class MNISTLogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._linear_layer = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self._linear_layer(X)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(model(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, loss_function, number_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(model(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring using `torch.optim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, loss_function, optimizer, number_samples, number_epochs=2, batch_size=64):\n",
    "    \n",
    "    number_batches = (number_samples - 1) // batch_size + 1\n",
    "    for epoch in range(number_epochs):\n",
    "        for batch in range(number_batches):\n",
    "            # forward pass\n",
    "            start = batch * batch_size\n",
    "            X = scaled_training_features[start:(start + batch_size)]\n",
    "            y = training_target[start:(start + batch_size)]\n",
    "            output = model(X)\n",
    "            loss = loss_function(output, y)\n",
    "            \n",
    "            # back propagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, F.cross_entropy, optim.SGD(model.parameters(), lr=0.5), number_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(model(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `TensorDataSet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data.TensorDataset(scaled_training_features, training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, loss_function, optimizer, number_samples, number_epochs=2, batch_size=64):\n",
    "    \n",
    "    number_batches = (number_samples - 1) // batch_size + 1\n",
    "    for epoch in range(number_epochs):\n",
    "        for batch in range(number_batches):\n",
    "            # forward pass\n",
    "            start = batch * batch_size\n",
    "            X, y = training_data[start:(start + batch_size)]\n",
    "            output = model(X)\n",
    "            loss = loss_function(output, y)\n",
    "            \n",
    "            # back propagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, F.cross_entropy, optim.SGD(model.parameters(), lr=0.5), number_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(model(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_loader = data.DataLoader(training_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, loss_function, optimizer, data_loader, number_epochs=2):\n",
    "    \n",
    "    for epoch in range(number_epochs):\n",
    "        for X, y in data_loader:\n",
    "            output = model(X)\n",
    "            loss = loss_function(output, y)\n",
    "            \n",
    "            # back propagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, F.cross_entropy, optim.SGD(model.parameters(), lr=0.5), training_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(model(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = data.TensorDataset(scaled_validation_features, validation_target)\n",
    "validation_data_loader = data.DataLoader(validation_data, batch_size=2*batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, loss_function, optimizer, training_data_loader, validation_data_loader=None, number_epochs=2):\n",
    "    \n",
    "    for epoch in range(number_epochs):\n",
    "        model.train()\n",
    "        for X, y in training_data_loader:\n",
    "            output = model(X)\n",
    "            loss = loss_function(output, y)\n",
    "            \n",
    "            # back propagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # compute validation loss after each training epoch\n",
    "        if validation_data_loader is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                batch_losses, batch_sizes = zip(*[(loss_function(model(X), y), len(X)) for X, y in validation_data_loader])\n",
    "                validation_loss = np.sum(np.multiply(batch_losses, batch_sizes)) / np.sum(batch_sizes)\n",
    "            print(epoch, validation_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, F.cross_entropy, optim.SGD(model.parameters(), lr=0.5), training_data_loader, validation_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switching to CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self._conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self._conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, 1, 28, 28) # implicit knowledge of MNIST data shape!\n",
    "        X = F.relu(self._conv1(X))\n",
    "        X = F.relu(self._conv2(X))\n",
    "        X = F.relu(self._conv3(X))\n",
    "        X = F.avg_pool2d(X, 4)\n",
    "        return X.view(-1, X.size(1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTCNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "fit(model, F.cross_entropy, optimizer, training_data_loader, validation_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, f):\n",
    "        super().__init__()\n",
    "        self._f = f\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self._f(X)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    LambdaLayer(lambda X: X.view(-1, 1, 28, 28)),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    LambdaLayer(lambda X: X.view(X.size(0), -1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model,\n",
    "    F.cross_entropy,\n",
    "    optim.SGD(model.parameters(), lr=0.1, momentum=0.9),\n",
    "    training_data_loader,\n",
    "    validation_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    \n",
    "    def __init__(self, data_loader, f):\n",
    "        self._data_loader = data_loader\n",
    "        self._f = f\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._data_loader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in iter(self._data_loader):\n",
    "            yield self._f(*batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = lambda X, y: (X.view(-1, 1, 28, 28), y)\n",
    "training_data_loader = WrappedDataLoader(training_data_loader, preprocess)\n",
    "validation_data_loader = WrappedDataLoader(validation_data_loader, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    LambdaLayer(lambda X: X.view(X.size(0), -1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model,\n",
    "    F.cross_entropy,\n",
    "    optim.SGD(model.parameters(), lr=0.1, momentum=0.9),\n",
    "    training_data_loader,\n",
    "    validation_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = lambda X, y: (X.view(-1, 1, 28, 28).to(\"cuda\"), y.to(\"cuda\"))\n",
    "training_data_loader = WrappedDataLoader(training_data_loader, preprocess)\n",
    "validation_data_loader = WrappedDataLoader(validation_data_loader, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model.to(\"cuda\"),\n",
    "    F.cross_entropy,\n",
    "    optim.SGD(model.parameters(), lr=0.1, momentum=0.9),\n",
    "    training_data_loader,\n",
    "    validation_data_loader,\n",
    "    number_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting to Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-train the model using the entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_target, training_features = mnist_arr[:, 0], mnist_arr[:, 1:]\n",
    "scaled_training_features = min_max_scaler.fit_transform(training_features)\n",
    "scaled_training_features_tensor = torch.tensor(scaled_training_features, dtype=torch.float32)\n",
    "training_target_tensor = torch.tensor(training_target)\n",
    "\n",
    "training_data = data.TensorDataset(scaled_training_features_tensor, training_target_tensor)\n",
    "training_data_loader = data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "wrapped_training_data_loader = WrappedDataLoader(training_data_loader, preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model.to(\"cuda\"),\n",
    "    F.cross_entropy,\n",
    "    optim.SGD(model.parameters(), lr=0.1, momentum=0.9),\n",
    "    wrapped_training_data_loader,\n",
    "    number_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission format for kaggle\n",
    "!head ../data/raw/mnist/sample_submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use trained model to make predictions using the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_features = np.loadtxt(\"../data/raw/mnist/test.csv\", delimiter=',', skiprows=1, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_testing_features = min_max_scaler.fit_transform(testing_features)\n",
    "scaled_testing_features = torch.tensor(scaled_testing_features, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(scaled_testing_features.view(-1, 1, 28, 28).to(\"cuda\"))\n",
    "predictions = torch.argmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_predictions, = predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df = pd.DataFrame({\"ImageId\": range(1, number_predictions + 1), \"Label\": predictions.cpu()})\n",
    "df.to_csv(f\"../data/kaggle-submissions/mnist/submission-{timestamp}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit digit-recognizer -f ../data/kaggle-submissions/mnist/submission-20190203-145624.csv -m \"My first ever Kaggle submission!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
