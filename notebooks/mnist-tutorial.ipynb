{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial assumes some basic knowledge of neural networks.  If you’re not, you can learn the basics of neural networks and PyTorch at [course.fast.ai](https://course.fast.ai/). \n",
    "\n",
    "* Tutorial materials is derived from [_What is torch.nn really?_](https://pytorch.org/tutorials/beginner/nn_tutorial.html) by Jeremy Howard, Rachel Thomas, Francisco Ingham.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import model_selection, preprocessing\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up an account with Kaggle (Optional, but recommended!)\n",
    "\n",
    "### 1. Register for an account with Kaggle\n",
    "\n",
    "In order to download Kaggle competition data you will first need to create a [Kaggle](https://www.kaggle.com/) account.\n",
    "\n",
    "### 2. Create an API key\n",
    "\n",
    "Once you have registered for a Kaggle account you will need to create some [API credentials](https://github.com/Kaggle/kaggle-api#api-credentials) in order to be able to use the `kaggle` CLI to download data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the MNIST data\n",
    "If you are using Binder to run this notebook, then the data has already been downloaded for you! If you are using Google Colab to run this notebook, then you will need to download the data before proceeding.\n",
    "\n",
    "## Downloading the data from Kaggle\n",
    "If you have a Kaggle account and API key, then you can provide your Kaggle username and API key in the cell below and execute the code to download the Kaggle [Digit Recognizer: Learn computer vision with the famous MNIST data](https://www.kaggle.com/c/digit-recognizer) competition data. **Before attempting to download the competition data you will need to login to your Kaggle account and accept the rules for this competition.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export KAGGLE_USERNAME=\"YOUR_USERNAME\"\n",
    "export KAGGLE_KEY=\"YOUR_API_KEY\"\n",
    "kaggle competitions download -c digit-recognizer -p ../data/raw/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data from GitHub\n",
    "If you are running this notebook using Google Colab but did not want to bother with setting up a Kaggle account and API key, then you can dowload the data from our GitHub repository by running the code in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_URL = \"https://raw.githubusercontent.com/kaust-vislab/pytorch-tutorials/master/data/raw/mnist/train.csv\"\n",
    "TEST_URL = \"https://raw.githubusercontent.com/kaust-vislab/pytorch-tutorials/master/data/raw/mnist/test.csv\"\n",
    "\n",
    "\n",
    "def fetch_mnist_data():\n",
    "    if not os.path.isdir(\"../data/raw/mnist/\"):\n",
    "        os.makedirs(\"../data/raw/mnist/\")\n",
    "    \n",
    "    with open(\"../data/raw/mnist/train.csv\", 'wb') as f:\n",
    "        response = requests.get(TRAIN_URL)\n",
    "        f.write(response.content)\n",
    "        \n",
    "    with open(\"../data/raw/mnist/test.csv\", 'wb') as f:\n",
    "        response = requests.get(TEST_URL)\n",
    "        f.write(response.content)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_mnist_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...or not!\n",
    "\n",
    "If you don't want to set up an account with Kaggle, then no worries! I have included the training and testing data sets for you.\n",
    "\n",
    "    ../data/raw/mnist/train.csv\n",
    "    ../data/raw/mnist/test/csv\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../data/raw/mnist/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_arr = np.loadtxt(\"../data/raw/mnist/train.csv\", delimiter=',', skiprows=1, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw features are between 0 and 255\n",
    "mnist_arr.min(), mnist_arr.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the MNIST data into training and validation sets\n",
    "\n",
    "Since Kaggle has already split the MNIST data set into training and testing data sets, we only need to split our training data set into training and validation data. We will use the validation data to make sure that we are not over-fitting our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prng = np.random.RandomState(42)\n",
    "training_arr, validation_arr = model_selection.train_test_split(mnist_arr, test_size=0.20, random_state=prng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_target, training_features = training_arr[:, 0], training_arr[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_target, validation_features = validation_arr[:, 0], validation_arr[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescale the raw data\n",
    "\n",
    "Data for individual pixels is stored as integers between 0 and 255. Neural network models work best when numerical features are scaled. To rescale the raw features we can use tools from the [Scikit-Learn preprocessing module](https://scikit-learn.org/stable/modules/preprocessing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_training_features = min_max_scaler.fit_transform(training_features)\n",
    "scaled_validation_features = min_max_scaler.fit_transform(validation_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out a training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,1)\n",
    "_ = ax.imshow(scaled_training_features[0].reshape((28, 28)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses `torch.tensor` rather than `numpy.ndarray` so we need to convert data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_target = torch.tensor(training_target)\n",
    "scaled_training_features = torch.tensor(scaled_training_features, dtype=torch.float32)\n",
    "\n",
    "validation_target = torch.tensor(validation_target)\n",
    "scaled_validation_features = torch.tensor(scaled_validation_features, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_training_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_training_features.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_training_features.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network from scratch\n",
    "\n",
    "First let's create a simple model using nothing but [PyTorch tensor operations](https://pytorch.org/docs/stable/tensors.html). PyTorch provides methods to create random or zero-filled tensors, which we will use to create our weights and bias for a simple linear model. These are just regular tensors, with one very special addition: we tell PyTorch that they require a gradient. This causes PyTorch to record all of the operations done on the tensor, so that it can calculate the gradient during back-propagation automatically!\n",
    "\n",
    "For the weights, we set `requires_grad` after the initialization, since we don’t want that step included in the gradient. (Note that a trailling `_` in PyTorch signifies that the operation is performed _in-place_.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_samples, number_features = scaled_training_features.shape\n",
    "\n",
    "# using Xavier initialization (divide weights by sqrt(number_features))\n",
    "weights = torch.randn(number_features, 10) / number_features**0.5\n",
    "weights.requires_grad_() # trailing underscore indicates in-place operation\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to PyTorch’s ability to calculate gradients automatically, we can use any standard Python function (or callable object) in a model! So we will start by writing a function to peform matrix multiplication and broadcasted addition called `linear_transformation`. We will also need an activation function, so we’ll write a function called `_log_softmax_activation` and use it. \n",
    "\n",
    "**N.B.** Although PyTorch provides lots of pre-written loss functions, activation functions, and so forth, you can easily write your own using plain python. PyTorch will even create fast GPU or vectorized CPU code for your function automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_transformation(X):\n",
    "    return X @ weights + bias\n",
    "\n",
    "def log_softmax_activation(X):\n",
    "    return X - X.exp().sum(-1).log().unsqueeze(-1)\n",
    "    \n",
    "def model(X):\n",
    "    Z = linear_transformation(X)\n",
    "    return log_softmax_activation(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, the `@` stands for the dot product operation. We will call our function on one batch of data (in this case, 64 images). Note that our predictions won’t be any better than random at this stage, since we start with random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "output = model(scaled_training_features[:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the `output` tensor contains not only the tensor values, but also a gradient function, `grad_fn`. We’ll use this later to do back propagation to update the model parameters.\n",
    "\n",
    "Let’s implement `negative_log_likelihood` to use as the loss function. Again, we can just use standard Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(output, target):\n",
    "    m, _ = output.shape\n",
    "    return -output[range(m), target].mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_log_likelihood(output, training_target[:batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s also implement a function to calculate the `accuracy` of our model: for each prediction, if the index with the largest value matches the target value, then the prediction was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    predictions = torch.argmax(output, dim=1)\n",
    "    return (predictions == target).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison purposes we can compute the accuracy of our model with randomly initialized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(output, training_target[:batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run a training loop. For each iteration, we will:\n",
    "\n",
    "* select a mini-batch of data (of size bs)\n",
    "* use the model to make predictions\n",
    "* calculate the loss\n",
    "* `loss.backward()` updates the gradients of the model.\n",
    "\n",
    "We now use these gradients to update the weights and bias (i.e., model parameters). We do this within the `torch.no_grad()` context manager, because we do not want these actions to be recorded for our next calculation of the gradient. You can read more about how PyTorch’s Autograd records operations [here](https://pytorch.org/docs/stable/notes/autograd.html).\n",
    "\n",
    "We then set the gradients to zero, so that we are ready for the next loop. Otherwise, our gradients would record a running tally of all the operations that had happened (i.e. loss.backward() adds the gradients to whatever is already stored, rather than replacing them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_epochs = 2\n",
    "number_batches = (number_samples - 1) // batch_size + 1\n",
    "\n",
    "learning_rate = 0.5\n",
    "for epoch in range(number_epochs):\n",
    "    for batch in range(number_batches):\n",
    "        \n",
    "        # forward pass\n",
    "        start = batch * batch_size\n",
    "        X = scaled_training_features[start:(start + batch_size)]\n",
    "        y = training_target[start:(start + batch_size)]\n",
    "        output = model(X)\n",
    "        loss = negative_log_likelihood(output, y)\n",
    "        \n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= learning_rate * weights.grad\n",
    "            bias -= learning_rate * bias.grad\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s it: we’ve created and trained a minimal neural network (in this case, a logistic regression, since we have no hidden layers) entirely from scratch! Let’s check the loss and accuracy and compare those to what we got earlier. We expect that the loss will have decreased and accuracy to have increased, and they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_log_likelihood(model(X), y), accuracy(model(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `torch.nn.functional`\n",
    "\n",
    "We will now refactor our code using PyTorch’s `nn` modules to make it more concise and flexible. The first and easiest step is to make our code shorter by replacing our hand-written activation and loss functions with those from `torch.nn.functional`. This module contains all the functions in the `torch.nn` library (whereas other parts of the library contain classes).\n",
    "\n",
    "Since we are using negative log likelihood loss and log softmax activation in this tutorial, we can use a single Pytorch, `torch.nn.functional.cross_entropy`, that combines the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_fn = F.cross_entropy\n",
    "model_fn = linear_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(model_fn(X), y), accuracy(model_fn(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `torch.nn.Module`\n",
    "\n",
    "Next up, we’ll use `torch.nn.Module` and `torch.nn.Parameter`, for a clearer and more concise training loop. In this case, we want to create a class that holds our weights, bias, and method for the forward step. `torch.nn.Module` has a number of attributes and methods (such as `parameters()` and `zero_grad()`) which we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class MNISTLogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._weights = nn.Parameter(torch.randn(784, 10) / 784**0.5)\n",
    "        self._bias = nn.Parameter(torch.zeros(10))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return X @ self._weights + self._bias\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we’re now using an object instead of just using a function, we first have to instantiate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = MNISTLogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the loss in the same way as before. Note that `torch.nn.Module` objects are used as if they are functions (i.e they are callable), but behind the scenes Pytorch will call the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(model_fn(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously in our training loop we had to update the values for each parameter by name and manually zero out the grads for each parameter separately.  With our refactoring we can take advantage of `model_fn.parameters()` and `model_fn.zero_grad()` (which are both defined by PyTorch for `torch.nn.Module` base class!) to make those steps more concise and less prone to the error of forgetting some of our parameters, particularly if we had a more complicated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_fit(model_fn, loss_fn, X_batch, y_batch):\n",
    "    # forward pass\n",
    "    loss = loss_fn(model_fn(X_batch), y_batch)\n",
    "\n",
    "    # back propagation\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for parameter in model_fn.parameters():\n",
    "            parameter -= learning_rate * parameter.grad\n",
    "        model_fn.zero_grad()\n",
    "\n",
    "\n",
    "def fit(model_fn, loss_fn, X, y, number_epochs=2, batch_size=64):\n",
    "    number_samples, _ = X.shape \n",
    "    number_batches = (number_samples - 1) // batch_size + 1\n",
    "    for epoch in range(number_epochs):\n",
    "        for batch in range(number_batches):\n",
    "            start = batch * batch_size\n",
    "            X_batch = X[start:(start + batch_size)]\n",
    "            y_batch = y[start:(start + batch_size)]\n",
    "            partial_fit(model_fn, loss_fn, X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn, loss_fn, scaled_training_features, training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(model_fn(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring using `nn.Linear`\n",
    "\n",
    "We continue to refactor our code. Instead of manually defining and initializing `self._weights` and `self._bias`, and calculating `X  @ self._weights + self._bias`, we will instead use the Pytorch class `torch.nn.Linear` to define a linear layer which does all that for us. Pytorch has many types of predefined layers that can greatly simplify our code, and often makes it faster too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class MNISTLogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._linear_layer = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self._linear_layer(X)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = MNISTLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(model_fn(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn, loss_fn, scaled_training_features, training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(model_fn(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring using `torch.optim`\n",
    "\n",
    "Pytorch also has a package with various optimization algorithms, `torch.optim`. We can use the step method from our optimizer to take a forward step, instead of manually updating each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_fit(model_fn, loss_fn, X_batch, y_batch, opt):\n",
    "    # forward pass\n",
    "    loss = loss_fn(model_fn(X_batch), y_batch)\n",
    "\n",
    "    # back propagation\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad() # don't forget to reset the gradient after each batch!\n",
    "\n",
    "        \n",
    "def fit(model_fn, loss_fn, X, y, opt, number_epochs=2, batch_size=64):\n",
    "    number_samples, _ = X.shape \n",
    "    number_batches = (number_samples - 1) // batch_size + 1\n",
    "    for epoch in range(number_epochs):\n",
    "        for batch in range(number_batches):\n",
    "            start = batch * batch_size\n",
    "            X_batch = X[start:(start + batch_size)]\n",
    "            y_batch = y[start:(start + batch_size)]\n",
    "            partial_fit(model_fn, loss_fn, X_batch, y_batch, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn, loss_fn, scaled_training_features, training_target, optim.SGD(model_fn.parameters(), lr=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(model_fn(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `TensorDataSet`\n",
    "\n",
    "PyTorch has an abstract `Dataset` class. A Dataset can be anything that has a `__len__` function (called by Python’s standard `len` function) and a `__getitem__` function as a way of indexing into it.\n",
    "\n",
    "PyTorch’s `TensorDataset` is a `Dataset` wrapping tensors. By defining a length and way of indexing, this also gives us a way to iterate, index, and slice along the first dimension of a tensor. This will make it easier to access both the independent and dependent variables in the same line as we train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data.TensorDataset(scaled_training_features, training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model_fn, loss_fn, data_set, number_samples, opt, number_epochs=2, batch_size=64):\n",
    "    number_batches = (number_samples - 1) // batch_size + 1\n",
    "    for epoch in range(number_epochs):\n",
    "        for batch in range(number_batches):\n",
    "            start = batch * batch_size\n",
    "            X_batch, y_batch = data_set[start:(start + batch_size)]\n",
    "            partial_fit(model_fn, loss_fn, X_batch, y_batch, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn, F.cross_entropy, training_data, number_samples, optim.SGD(model_fn.parameters(), lr=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(model_fn(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `DataLoader`\n",
    "\n",
    "Pytorch’s `DataLoader` is responsible for managing batches. You can create a `DataLoader` from any `Dataset`. `DataLoader` makes it easier to iterate over batches. Rather than having to use `train_ds[i*bs : i*bs+bs]`, the `DataLoader` gives us each minibatch automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_loader = data.DataLoader(training_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model_fn, loss_fn, data_loader, opt, number_epochs=2, batch_size=64):\n",
    "    for epoch in range(number_epochs):\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            partial_fit(model_fn, loss_fn, X_batch, y_batch, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn, F.cross_entropy, training_data_loader, optim.SGD(model_fn.parameters(), lr=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(model_fn(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to Pytorch’s `torch.nn.Module`, `torch.nn.Parameter`, `Dataset`, and `DataLoader`, our training loop is now dramatically smaller and easier to understand. Let’s now try to add the basic features necessary to create effecive models in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Validation\n",
    "\n",
    "In the first part of this tutorial, we were just trying to get a reasonable training loop set up for use on our training data. In reality, you always should also have a validation set, in order to identify if you are overfitting.\n",
    "\n",
    "Shuffling the training data is important to prevent correlation between batches and overfitting. On the other hand, the validation loss will be identical whether we shuffle the validation set or not. Since shuffling takes extra time, it makes no sense to shuffle the validation data.\n",
    "\n",
    "We’ll use a batch size for the validation set that is twice as large as that for the training set. This is because the validation set does not need backpropagation and thus takes less memory (it doesn’t need to store the gradients). We take advantage of this to use a larger batch size and compute the loss more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = data.TensorDataset(scaled_validation_features, validation_target)\n",
    "validation_data_loader = data.DataLoader(validation_data, batch_size=2*batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model_fn, loss_fn, training_data_loader, opt, validation_data_loader=None, number_epochs=2):\n",
    "    \n",
    "    for epoch in range(number_epochs):\n",
    "        model_fn.train()\n",
    "        for X_batch, y_batch in training_data_loader:\n",
    "            partial_fit(model_fn, loss_fn, X_batch, y_batch, opt)\n",
    "        \n",
    "        # compute validation loss after each training epoch\n",
    "        if validation_data_loader is not None:\n",
    "            model_fn.eval()\n",
    "            with torch.no_grad():\n",
    "                batch_losses, batch_sizes = zip(*[(loss_fn(model_fn(X), y), len(X)) for X, y in validation_data_loader])\n",
    "                validation_loss = np.sum(np.multiply(batch_losses, batch_sizes)) / np.sum(batch_sizes)\n",
    "            print(epoch, validation_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn, F.cross_entropy, training_data_loader, optim.SGD(model_fn.parameters(), lr=0.5), validation_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switching to CNN\n",
    "\n",
    "We are now going to build our neural network with three convolutional layers. Because none of the functions in the previous section assume anything about the model form, we’ll be able to use them to train a CNN without any modification!\n",
    "\n",
    "We will use Pytorch’s predefined `Conv2d` class as our convolutional layer. We define a CNN with 3 convolutional layers. Each convolution is followed by a `ReLU`. At the end, we perform an average pooling. (Note that `view` is PyTorch’s version of NumPy’s `reshape`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self._conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self._conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, 1, 28, 28) # implicit knowledge of MNIST data shape!\n",
    "        X = F.relu(self._conv1(X))\n",
    "        X = F.relu(self._conv2(X))\n",
    "        X = F.relu(self._conv3(X))\n",
    "        X = F.avg_pool2d(X, 4)\n",
    "        return X.view(-1, X.size(1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = MNISTCNN()\n",
    "opt = optim.SGD(model_fn.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "fit(model_fn, F.cross_entropy, training_data_loader, opt, validation_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `nn.Sequential`\n",
    "\n",
    "`torch.nn` has another handy class we can use to simply our code: `Sequential`. A `Sequential` object runs each of the modules contained within it, in a sequential manner. This is a simpler way of writing our neural network.\n",
    "\n",
    "To take advantage of this, we need to be able to easily define a custom layer from a given function. For instance, PyTorch doesn’t have a view layer, and we need to create one for our network. `LambdaLayer` will create a layer that we can then use when defining a network with `Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, f):\n",
    "        super().__init__()\n",
    "        self._f = f\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self._f(X)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = nn.Sequential(\n",
    "    LambdaLayer(lambda X: X.view(-1, 1, 28, 28)),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    LambdaLayer(lambda X: X.view(X.size(0), -1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn,\n",
    "    F.cross_entropy,\n",
    "    training_data_loader,\n",
    "    optim.SGD(model_fn.parameters(), lr=0.1, momentum=0.9),\n",
    "    validation_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping DataLoader\n",
    "\n",
    "\n",
    "Our CNN is fairly concise, but it only works with MNIST, because:\n",
    "\n",
    "1. It assumes the input is a 28*28 long vector\n",
    "2. It assumes that the final CNN grid size is 4*4 (since that’s the average pooling kernel size we used)\n",
    "\n",
    "Let’s get rid of these two assumptions, so our model works with any 2d single channel image. First, we can remove the initial Lambda layer but moving the data preprocessing into a generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    \n",
    "    def __init__(self, data_loader, f):\n",
    "        self._data_loader = data_loader\n",
    "        self._f = f\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._data_loader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in iter(self._data_loader):\n",
    "            yield self._f(*batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = lambda X, y: (X.view(-1, 1, 28, 28), y)\n",
    "training_data_loader = WrappedDataLoader(training_data_loader, preprocess)\n",
    "validation_data_loader = WrappedDataLoader(validation_data_loader, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can replace `nn.AvgPool2d` with `nn.AdaptiveAvgPool2d`, which allows us to define the size of the output tensor we want, rather than the input tensor we have. As a result, our model will work with any size input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    LambdaLayer(lambda X: X.view(X.size(0), -1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model,\n",
    "    F.cross_entropy,\n",
    "    training_data_loader,\n",
    "    optim.SGD(model.parameters(), lr=0.1, momentum=0.9),\n",
    "    validation_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPU\n",
    "\n",
    "Training models within Google Colab we can take advantage of free GPUs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = lambda X, y: (X.view(-1, 1, 28, 28).to(device), y.to(device))\n",
    "training_data_loader = WrappedDataLoader(training_data_loader, preprocess)\n",
    "validation_data_loader = WrappedDataLoader(validation_data_loader, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model.to(device),\n",
    "    F.cross_entropy,\n",
    "    training_data_loader,\n",
    "    optim.SGD(model.parameters(), lr=0.1, momentum=0.9),\n",
    "    validation_data_loader,\n",
    "    number_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your own model!\n",
    "\n",
    "Using the above code as a template, try and create your own model function to classify the MNIST data. How accurate is your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn.to(device),\n",
    "    F.cross_entropy,\n",
    "    training_data_loader,\n",
    "    optim.SGD(model_fn.parameters(), lr=0.1, momentum=0.9),\n",
    "    validation_data_loader,\n",
    "    number_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting to Kaggle\n",
    "\n",
    "If you have created a Kaggle account, then you can submit your model's predictions to Kaggle and see how you stack up against your peers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-train the model using the entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_target, training_features = mnist_arr[:, 0], mnist_arr[:, 1:]\n",
    "scaled_training_features = min_max_scaler.fit_transform(training_features)\n",
    "scaled_training_features_tensor = torch.tensor(scaled_training_features, dtype=torch.float32)\n",
    "training_target_tensor = torch.tensor(training_target)\n",
    "\n",
    "training_data = data.TensorDataset(scaled_training_features_tensor, training_target_tensor)\n",
    "training_data_loader = data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "wrapped_training_data_loader = WrappedDataLoader(training_data_loader, preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model.to(device),\n",
    "    F.cross_entropy,\n",
    "    wrapped_training_data_loader,\n",
    "    optim.SGD(model.parameters(), lr=0.1, momentum=0.9),\n",
    "    number_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use trained model to make predictions using the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_features = np.loadtxt(\"../data/raw/mnist/test.csv\", delimiter=',', skiprows=1, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_testing_features = min_max_scaler.fit_transform(testing_features)\n",
    "scaled_testing_features = torch.tensor(scaled_testing_features, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(scaled_testing_features.view(-1, 1, 28, 28).to(device))\n",
    "predictions = torch.argmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission format for kaggle\n",
    "!head ../data/raw/mnist/sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "number_predictions, = predictions.shape\n",
    "df = pd.DataFrame({\"ImageId\": range(1, number_predictions + 1), \"Label\": predictions.cpu()})\n",
    "df.to_csv(f\"../data/kaggle-submissions/mnist/submission-{timestamp}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to Kaggle!\n",
    "\n",
    "Once you have successfully submited your predictions then you can check the [Digit-Recognizer competition](https://www.kaggle.com/c/digit-recognizer) website and see how well your best model compares to your peers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export KAGGLE_USERNAME=\"YOUR_USERNAME\"\n",
    "export KAGGLE_KEY=\"YOUR_API_KEY\"\n",
    "!kaggle competitions submit digit-recognizer -f ../data/kaggle-submissions/mnist/submission-20190321-143216.csv -m \"My first ever Kaggle submission!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
