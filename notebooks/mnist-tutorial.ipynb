{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction\n",
    "\n",
    "MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike. \n",
    "\n",
    "In this tutorial, your goal is to correctly identify digits from the Kaggle MNIST dataset of tens of thousands of handwritten images. We will walk through the development of several standard deep learning pipelines using PyTorch that are capable of correctly identifying digits from the MNIST dataset. We will then see how to customize standard deep learning pipelines to improve model performance. After successfully training a custom model, we will see how to submit the model's predictions to Kaggle for scoring.\n",
    "\n",
    "This tutorial assumes some basic knowledge of neural networks.  If you’re not already familiar with neural networks, then you can learn the basics concepts behind neural networks (and PyTorch!) at [course.fast.ai](https://course.fast.ai/). \n",
    "\n",
    "* Tutorial materials are derived from [_What is torch.nn really?_](https://pytorch.org/tutorials/beginner/nn_tutorial.html) by Jeremy Howard, Rachel Thomas, Francisco Ingham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import decomposition, ensemble, manifold, metrics, model_selection, pipeline, preprocessing\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up an account with Kaggle (Optional, but recommended!)\n",
    "\n",
    "### 1. Register for an account with Kaggle\n",
    "\n",
    "In order to download Kaggle competition data you will first need to create a [Kaggle](https://www.kaggle.com/) account.\n",
    "\n",
    "### 2. Create an API key\n",
    "\n",
    "Once you have registered for a Kaggle account you will need to create some [API credentials](https://github.com/Kaggle/kaggle-api#api-credentials) in order to be able to use the `kaggle` CLI to download data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the MNIST data\n",
    "If you are using Binder to run this notebook, then the data has already been downloaded for you! If you are using Google Colab to run this notebook, then you will need to download the data before proceeding.\n",
    "\n",
    "## Downloading the data from Kaggle\n",
    "If you have a Kaggle account and API key, then you can provide your Kaggle username and API key in the cell below and execute the code to download the Kaggle [Digit Recognizer: Learn computer vision with the famous MNIST data](https://www.kaggle.com/c/digit-recognizer) competition data. **Before attempting to download the competition data you will need to login to your Kaggle account and accept the rules for this competition.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export KAGGLE_USERNAME=\"YOUR_USERNAME\"\n",
    "export KAGGLE_KEY=\"YOUR_API_KEY\"\n",
    "kaggle competitions download -c digit-recognizer -p ../data/raw/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data from GitHub\n",
    "If you are running this notebook using Google Colab but did not want to bother with setting up a Kaggle account and API key, then you can dowload the data from our GitHub repository by running the code in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "TRAIN_URL = \"https://raw.githubusercontent.com/kaust-vislab/pytorch-tutorials/master/data/raw/mnist/train.csv\"\n",
    "TEST_URL = \"https://raw.githubusercontent.com/kaust-vislab/pytorch-tutorials/master/data/raw/mnist/test.csv\"\n",
    "SAMPLE_SUBMISSION_URL = \"https://raw.githubusercontent.com/kaust-vislab/pytorch-tutorials/master/data/raw/mnist/sample_submission.csv\"\n",
    "\n",
    "\n",
    "def fetch_mnist_data():\n",
    "    if not os.path.isdir(\"../data/raw/mnist/\"):\n",
    "        os.makedirs(\"../data/raw/mnist/\")\n",
    "    \n",
    "    with open(\"../data/raw/mnist/train.csv\", 'wb') as f:\n",
    "        response = requests.get(TRAIN_URL)\n",
    "        f.write(response.content)\n",
    "        \n",
    "    with open(\"../data/raw/mnist/test.csv\", 'wb') as f:\n",
    "        response = requests.get(TEST_URL)\n",
    "        f.write(response.content)\n",
    "    \n",
    "    with open(\"../data/raw/mnist/sample_submission.csv\", 'wb') as f:\n",
    "        response = requests.get(SAMPLE_SUBMISSION_URL)\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_mnist_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../data/raw/mnist/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_arr = np.loadtxt(\"../data/raw/mnist/train.csv\", delimiter=',', skiprows=1, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw features are between 0 and 255\n",
    "mnist_arr.min(), mnist_arr.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescale the raw data\n",
    "\n",
    "Data for individual pixels is stored as integers between 0 and 255. Neural network models work best when numerical features are scaled. To rescale the raw features we can use tools from the [Scikit-Learn preprocessing module](https://scikit-learn.org/stable/modules/preprocessing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_target, training_features = mnist_arr[:, 0], mnist_arr[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_training_features = min_max_scaler.fit_transform(training_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out a training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,1)\n",
    "_ = ax.imshow(scaled_training_features[0].reshape((28, 28)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training samples using PCA\n",
    "\n",
    "[Principal Components Analysis (PCA)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) can be used as a visualization tool to see if there are any obvious patterns in the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prng = np.random.RandomState(42)\n",
    "pca = decomposition.PCA(n_components=2, random_state=_prng)\n",
    "transformed_training_features = pca.fit_transform(scaled_training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "_ = ax.scatter(transformed_training_features[:,0], transformed_training_features[:,1], c=training_target, alpha=0.05)\n",
    "ax.set_xlabel(\"Component 1\")\n",
    "ax.set_ylabel(\"Component 2\")\n",
    "ax.set_title(\"PCA\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training sample using t-SNE\n",
    "\n",
    "[t-distributed Stochastic Neighbor Embedding (t-SNE)](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE) is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data.\n",
    "\n",
    "It is highly recommended to use another dimensionality reduction method (e.g. PCA) to reduce the number of dimensions to a reasonable amount if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial PCA to determine n_components that capture 95% of sample variance\n",
    "_prng = np.random.RandomState(42)\n",
    "pca = decomposition.PCA(random_state=_prng)\n",
    "pca.fit_transform(scaled_training_features)\n",
    "n_components = np.sum(pca.explained_variance_ratio_.cumsum() < 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suggested above we will go ahead and use an initial PCA step in our embedding pipeline to reduce the overall number of features which will speed up convergence of the t-SNE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prng = np.random.RandomState(42)\n",
    "\n",
    "embedding_pipeline = pipeline.make_pipeline(\n",
    "    decomposition.PCA(n_components, random_state=_prng),\n",
    "    manifold.TSNE(n_components=2, random_state=_prng)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_training_features = embedding_pipeline.fit_transform(scaled_training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "_ = ax.scatter(transformed_training_features[:,0], transformed_training_features[:,1], c=training_target, alpha=0.05)\n",
    "ax.set_xlabel(\"Component 1\")\n",
    "ax.set_ylabel(\"Component 2\")\n",
    "ax.set_title(\"t-SNE\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical ML Benchmark Model\n",
    "\n",
    "To provide a point of comparison for our neural network models, let's use PCA to peform dimensionality reduction and then train a Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial PCA to determine n_components that capture 95% of sample variance\n",
    "_prng = np.random.RandomState(42)\n",
    "pca = decomposition.PCA(random_state=_prng)\n",
    "pca.fit_transform(scaled_training_features)\n",
    "pca.set_params(n_components=np.sum(pca.explained_variance_ratio_.cumsum() < 0.95))\n",
    "\n",
    "# second PCA fit to compute the transformed features\n",
    "transformed_training_features = pca.fit_transform(scaled_training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a reasonable classifier using CV\n",
    "clf = ensemble.RandomForestClassifier(n_estimators=100, random_state=_prng)\n",
    "accuracy_scores = model_selection.cross_val_score(clf,\n",
    "                                                  transformed_training_features,\n",
    "                                                  training_target,\n",
    "                                                  scoring=\"accuracy\",\n",
    "                                                  cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain the classifier using the entire dataset\n",
    "clf.fit(transformed_training_features, training_target)\n",
    "\n",
    "# load the testing features (note we use transform method and NOT fit_transform!)\n",
    "_testing_features = np.loadtxt(\"../data/raw/mnist/test.csv\", delimiter=',', skiprows=1, dtype=np.int64)\n",
    "_scaled_testing_features = min_max_scaler.transform(_testing_features)\n",
    "_transformed_testing_features = pca.transform(_scaled_testing_features)\n",
    "\n",
    "# make predictions \n",
    "predictions = clf.predict(_transformed_testing_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission format for kaggle\n",
    "!head ../data/raw/mnist/sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if not os.path.isdir(\"../data/kaggle-submissions/mnist/\"):\n",
    "    os.makedirs(\"../data/kaggle-submissions/mnist/\")\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "number_predictions, = predictions.shape\n",
    "df = pd.DataFrame({\"ImageId\": range(1, number_predictions + 1), \"Label\": predictions})\n",
    "df.to_csv(f\"../data/kaggle-submissions/mnist/submission-{timestamp}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to Kaggle!\n",
    "\n",
    "Once you have successfully submited your predictions then you can check the [Digit-Recognizer competition](https://www.kaggle.com/c/digit-recognizer) website and see how well your best model compares to your peers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export KAGGLE_USERNAME=\"YOUR_USERNAME\"\n",
    "export KAGGLE_KEY=\"YOUR_API_KEY\"\n",
    "kaggle competitions submit digit-recognizer \\\n",
    "  -f $(ls ../data/kaggle-submissions/mnist/submission-*.csv | tail -n 1) \\\n",
    "  -m \"Untuned PCA-RandomForestClassifier benchmark pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network from scratch\n",
    "\n",
    "## Split the MNIST data into training and validation sets\n",
    "\n",
    "Since Kaggle has already split the MNIST data set into training and testing data sets, we only need to split our training data set into training and validation data. We will use the validation data to make sure that we are not over-fitting our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prng = np.random.RandomState(42)\n",
    "training_arr, validation_arr = model_selection.train_test_split(mnist_arr, test_size=0.20, random_state=prng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first column is the label (i.e., target), remaining columns are pixel values (i.e., features)\n",
    "training_target, training_features = training_arr[:, 0], training_arr[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_target, validation_features = validation_arr[:, 0], validation_arr[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next let's create a simple model using nothing but [PyTorch tensor operations](https://pytorch.org/docs/stable/tensors.html). PyTorch uses `torch.tensor` rather than `numpy.ndarray` so we need to convert data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_target = torch.tensor(training_target)\n",
    "scaled_training_features = torch.tensor(scaled_training_features, dtype=torch.float32)\n",
    "\n",
    "validation_target = torch.tensor(validation_target)\n",
    "scaled_validation_features = torch.tensor(scaled_validation_features, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_training_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides methods to create random or zero-filled tensors, which we will use to create our weights and bias for a simple linear model. These are just regular tensors, with one very special addition: we tell PyTorch that they require a gradient. This causes PyTorch to record all of the operations done on the tensor, so that it can calculate the gradient during back-propagation automatically!\n",
    "\n",
    "For the weights, we set `requires_grad` after the initialization, since we don’t want that step included in the gradient. (Note that a trailling `_` in PyTorch signifies that the operation is performed _in-place_.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_samples, number_features = scaled_training_features.shape\n",
    "\n",
    "# using Xavier initialization (divide weights by sqrt(number_features))\n",
    "weights = torch.randn(number_features, 10) / number_features**0.5\n",
    "weights.requires_grad_() # trailing underscore indicates in-place operation\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to PyTorch’s ability to calculate gradients automatically, we can use any standard Python function (or callable object) in a model! So we will start by writing a function to peform matrix multiplication and broadcasted addition called `linear_transformation`. We will also need an activation function, so we’ll write a function called `log_softmax_activation` and use it. \n",
    "\n",
    "**N.B.** Although PyTorch provides lots of pre-written loss functions, activation functions, and so forth, you can easily write your own using plain python. PyTorch will even create fast GPU or vectorized CPU code for your function automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_transformation(X):\n",
    "    return X @ weights + bias\n",
    "\n",
    "def log_softmax_activation(X):\n",
    "    return X - X.exp().sum(-1).log().unsqueeze(-1)\n",
    "    \n",
    "def logistic_regression(X):\n",
    "    Z = linear_transformation(X)\n",
    "    return log_softmax_activation(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, the `@` stands for the dot product operation. We will call our function on one batch of data (in this case, 64 images). Note that our predictions won’t be any better than random at this stage, since we start with random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "output = logistic_regression(scaled_training_features[:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the `output` tensor contains not only the tensor values, but also a gradient function, `grad_fn`. We’ll use this later to do back propagation to update the model parameters.\n",
    "\n",
    "Let’s implement `negative_log_likelihood` to use as the loss function. Again, we can just use standard Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(output, target):\n",
    "    m, _ = output.shape\n",
    "    return -output[range(m), target].mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_log_likelihood(output, training_target[:batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s also implement a function to calculate the `accuracy` of our model: for each prediction, if the index with the largest value matches the target value, then the prediction was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    predictions = torch.argmax(output, dim=1)\n",
    "    return (predictions == target).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison purposes we can compute the accuracy of our model with randomly initialized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(output, training_target[:batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run a training loop. For each iteration, we will:\n",
    "\n",
    "* select a mini-batch of data (of size `batch_size`)\n",
    "* use the model to make predictions\n",
    "* calculate the loss\n",
    "* `loss.backward()` updates the gradients of the model.\n",
    "\n",
    "We now use these gradients to update the weights and bias (i.e., model parameters). We do this within the `torch.no_grad()` context manager, because we do not want these actions to be recorded for our next calculation of the gradient. You can read more about how PyTorch’s Autograd records operations [here](https://pytorch.org/docs/stable/notes/autograd.html).\n",
    "\n",
    "We then set the gradients to zero, so that we are ready for the next loop. Otherwise, our gradients would record a running tally of all the operations that had happened (i.e. loss.backward() adds the gradients to whatever is already stored, rather than replacing them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = logistic_regression\n",
    "loss_fn = negative_log_likelihood\n",
    "\n",
    "number_epochs = 2\n",
    "number_batches = (number_samples - 1) // batch_size + 1\n",
    "\n",
    "learning_rate = 0.5\n",
    "for epoch in range(number_epochs):\n",
    "    for batch in range(number_batches):\n",
    "        \n",
    "        # forward pass\n",
    "        start = batch * batch_size\n",
    "        X = scaled_training_features[start:(start + batch_size)]\n",
    "        y = training_target[start:(start + batch_size)]\n",
    "        loss = loss_fn(model_fn(X), y)\n",
    "        \n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= learning_rate * weights.grad\n",
    "            bias -= learning_rate * bias.grad\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s it: we’ve created and trained a minimal neural network (in this case, a logistic regression, since we have no hidden layers) entirely from scratch! Let’s check the loss and accuracy and compare those to what we got earlier. We expect that the loss will have decreased and accuracy to have increased, and they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy(model_fn(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `torch.nn.functional`\n",
    "\n",
    "We will now refactor our code using [torch.nn](https://pytorch.org/docs/stable/nn.html) modules to make it more concise and flexible. The first and easiest step is to make our code shorter by replacing our hand-written activation and loss functions with those from [torch.nn.functional](https://pytorch.org/docs/stable/nn.html#torch-nn-functional).\n",
    "\n",
    "Since we are using negative log likelihood loss and log softmax activation in this tutorial, we can use [torch.nn.functional.cross_entropy](https://pytorch.org/docs/stable/nn.html#cross-entropy) which combines the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(linear_transformation(X), y), accuracy(linear_transformation(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `torch.nn.Module`\n",
    "\n",
    "Next up, we’ll use [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#module) and [torch.nn.Parameter](https://pytorch.org/docs/stable/nn.html#parameters), for a clearer and more concise training loop. In this case, we want to create a class that holds our weights, bias, and method for the forward step. `torch.nn.Module` has a number of attributes and methods (such as `parameters()` and `zero_grad()`) which we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class MNISTLogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._weights = nn.Parameter(torch.randn(784, 10) / 784**0.5)\n",
    "        self._bias = nn.Parameter(torch.zeros(10))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return X @ self._weights + self._bias\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we’re now using an object instead of just using a function, we first have to instantiate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = MNISTLogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the loss in the same way as before. Note that `torch.nn.Module` objects are used as if they are functions (i.e they are callable), but behind the scenes Pytorch will call the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(model_fn(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously in our training loop we had to update the values for each parameter by name and manually zero out the grads for each parameter separately.  With our refactoring we can take advantage of `model_fn.parameters()` and `model_fn.zero_grad()` (which are both defined by PyTorch for `torch.nn.Module` base class!) to make those steps more concise and less prone to the error of forgetting some of our parameters, particularly if we had a more complicated model.\n",
    "\n",
    "In order to facilitate re-use and continued refactoring, we can encapsulate the logic of our deep learning pipeline in the following functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_fit(model_fn, loss_fn, X_batch, y_batch):\n",
    "    # forward pass\n",
    "    loss = loss_fn(model_fn(X_batch), y_batch)\n",
    "\n",
    "    # back propagation\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for parameter in model_fn.parameters():\n",
    "            parameter -= learning_rate * parameter.grad\n",
    "        model_fn.zero_grad()\n",
    "\n",
    "\n",
    "def fit(model_fn, loss_fn, X, y, number_epochs=2, batch_size=64):\n",
    "    number_samples, _ = X.shape \n",
    "    number_batches = (number_samples - 1) // batch_size + 1\n",
    "    for epoch in range(number_epochs):\n",
    "        for batch in range(number_batches):\n",
    "            start = batch * batch_size\n",
    "            X_batch = X[start:(start + batch_size)]\n",
    "            y_batch = y[start:(start + batch_size)]\n",
    "            partial_fit(model_fn, loss_fn, X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = MNISTLogisticRegression()\n",
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn, loss_fn, scaled_training_features, training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(model_fn(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring using `torch.nn.Linear`\n",
    "\n",
    "Instead of defining and initializing `self._weights` and `self._bias`, and calculating `X  @ self._weights + self._bias`, we will instead use the Pytorch class [torch.nn.Linear](https://pytorch.org/docs/stable/nn.html#linear) to define a linear layer which does all that for us. Pytorch has many types of predefined layers that can greatly simplify our code, and since the library code is highly optimized using PyTorch's predefined layers often makes our code faster too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class MNISTLogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._linear_layer = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self._linear_layer(X)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = MNISTLogisticRegression()\n",
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn, loss_fn, scaled_training_features, training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(model_fn(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring using `torch.optim`\n",
    "\n",
    "Pytorch also has a package with various optimization algorithms, [torch.optim](https://pytorch.org/docs/stable/optim.html). We can use the step method from our optimizer to take a forward step, instead of manually updating each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_fit(model_fn, loss_fn, X_batch, y_batch, opt):\n",
    "    # forward pass\n",
    "    loss = loss_fn(model_fn(X_batch), y_batch)\n",
    "\n",
    "    # back propagation\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad() # don't forget to reset the gradient after each batch!\n",
    "\n",
    "        \n",
    "def fit(model_fn, loss_fn, X, y, opt, number_epochs=2, batch_size=64):\n",
    "    number_samples, _ = X.shape \n",
    "    number_batches = (number_samples - 1) // batch_size + 1\n",
    "    for epoch in range(number_epochs):\n",
    "        for batch in range(number_batches):\n",
    "            start = batch * batch_size\n",
    "            X_batch = X[start:(start + batch_size)]\n",
    "            y_batch = y[start:(start + batch_size)]\n",
    "            partial_fit(model_fn, loss_fn, X_batch, y_batch, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = MNISTLogisticRegression()\n",
    "loss_fn = F.cross_entropy\n",
    "opt = optim.SGD(model_fn.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn, loss_fn, scaled_training_features, training_target, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(model_fn(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `torch.utils.data.TensorDataSet`\n",
    "\n",
    "The [torch.utils.data](https://pytorch.org/docs/stable/data.html#module-torch.utils.data) module contains a number of useful classes that we can use to further simplify our code. PyTorch has an abstract `Dataset` class. A Dataset can be anything that has a `__len__` function (called by Python’s standard `len` function) and a `__getitem__` function as a way of indexing into it.\n",
    "\n",
    "PyTorch’s `TensorDataset` is a `Dataset` wrapping tensors. By defining a length and way of indexing, this also gives us a way to iterate, index, and slice along the first dimension of a tensor. This will make it easier to access both the independent and dependent variables in the same line as we train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model_fn, loss_fn, data_set, number_samples, opt, number_epochs=2, batch_size=64):\n",
    "    number_batches = (number_samples - 1) // batch_size + 1\n",
    "    for epoch in range(number_epochs):\n",
    "        for batch in range(number_batches):\n",
    "            start = batch * batch_size\n",
    "            X_batch, y_batch = data_set[start:(start + batch_size)]\n",
    "            partial_fit(model_fn, loss_fn, X_batch, y_batch, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = MNISTLogisticRegression()\n",
    "loss_fn = F.cross_entropy\n",
    "training_data_set = data.TensorDataset(scaled_training_features, training_target)\n",
    "opt = optim.SGD(model_fn.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the annoying dependence on number of samples!\n",
    "fit(model_fn, loss_fn, training_data_set, number_samples, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(model_fn(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `torch.utils.data.DataLoader`\n",
    "\n",
    "Pytorch’s `DataLoader` is responsible for managing batches. You can create a `DataLoader` from any `Dataset`. `DataLoader` makes it easier to iterate over batches. Rather than having to use `data_set[start:(start + batch_size)]`, the `DataLoader` gives us each minibatch automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model_fn, loss_fn, data_loader, opt, number_epochs=2, batch_size=64):\n",
    "    for epoch in range(number_epochs):\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            partial_fit(model_fn, loss_fn, X_batch, y_batch, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = MNISTLogisticRegression()\n",
    "loss_fn = F.cross_entropy\n",
    "training_data_loader = data.DataLoader(training_data_set, batch_size=batch_size, shuffle=True)\n",
    "opt = optim.SGD(model_fn.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we no longer have the annoying dependency on number of samples!\n",
    "fit(model_fn, loss_fn, training_data_loader, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(model_fn(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to Pytorch’s `torch.nn.Module`, `torch.nn.Parameter`, `Dataset`, and `DataLoader`, our training loop is now dramatically smaller and easier to understand. Let’s now try to add the basic features necessary to create effecive models in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Validation\n",
    "\n",
    "In the first part of this tutorial, we were just trying to get a reasonable training loop set up for use on our training data. In reality, you always should also have a validation set, in order to identify if you are overfitting.\n",
    "\n",
    "Shuffling the training data is important to prevent correlation between batches and overfitting. On the other hand, the validation loss will be identical whether we shuffle the validation set or not. Since shuffling takes extra time, it makes no sense to shuffle the validation data.\n",
    "\n",
    "We’ll use a batch size for the validation set that is twice as large as that for the training set. This is because the validation set does not need backpropagation and thus takes less memory (it doesn’t need to store the gradients). We take advantage of this to use a larger batch size and compute the loss more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model_fn, loss_fn, training_data_loader, opt, validation_data_loader=None, number_epochs=2):\n",
    "    \n",
    "    for epoch in range(number_epochs):\n",
    "        model_fn.train()\n",
    "        for X_batch, y_batch in training_data_loader:\n",
    "            partial_fit(model_fn, loss_fn, X_batch, y_batch, opt)\n",
    "        \n",
    "        # compute validation loss after each training epoch\n",
    "        if validation_data_loader is not None:\n",
    "            model_fn.eval()\n",
    "            with torch.no_grad():\n",
    "                batch_losses, batch_sizes = zip(*[(loss_fn(model_fn(X), y), len(X)) for X, y in validation_data_loader])\n",
    "                validation_loss = np.sum(np.multiply(batch_losses, batch_sizes)) / np.sum(batch_sizes)\n",
    "            print(f\"Training epoch: {epoch}, Validation loss: {validation_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = MNISTLogisticRegression()\n",
    "loss_fn = F.cross_entropy\n",
    "training_data_loader = data.DataLoader(training_data_set, batch_size=batch_size, shuffle=True)\n",
    "opt = optim.SGD(model_fn.parameters(), lr=0.5)\n",
    "\n",
    "_validation_data_set = data.TensorDataset(scaled_validation_features, validation_target)\n",
    "validation_data_loader = data.DataLoader(_validation_data_set, batch_size=2*batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn, loss_fn, training_data_loader, opt, validation_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switching to CNN\n",
    "\n",
    "We are now going to build our neural network with three convolutional layers. Because none of the functions in the previous section assume anything about the model form, we’ll be able to use them to train a CNN without any modification!\n",
    "\n",
    "We will use Pytorch’s predefined [torch.nn.Conv2d](https://pytorch.org/docs/stable/nn.html#conv2d) class as our convolutional layer. We define a CNN with 3 convolutional layers. Each convolution is followed by a [torch.nn.functional.relu](https://pytorch.org/docs/stable/nn.html#id26) non-linear activation function. At the end, we perform an average pooling using [torch.nn.functional.avg_pool2d](https://pytorch.org/docs/stable/nn.html#avg-pool2d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self._conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self._conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, 1, 28, 28) # implicit knowledge of MNIST data shape!\n",
    "        X = F.relu(self._conv1(X))\n",
    "        X = F.relu(self._conv2(X))\n",
    "        X = F.relu(self._conv3(X))\n",
    "        X = F.avg_pool2d(X, 4)\n",
    "        return X.view(-1, X.size(1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = MNISTCNN()\n",
    "opt = optim.SGD(model_fn.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we can re-use the loss function as well as trainig and validation data loaders\n",
    "fit(model_fn, loss_fn, training_data_loader, opt, validation_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `torch.nn.Sequential`\n",
    "\n",
    "PyTorch has another handy class we can use to simply our code: [torch.nn.Sequential](https://pytorch.org/docs/stable/nn.html#sequential). A `Sequential` object runs each of the modules contained within it, in a sequential manner. This is a simpler way of writing our neural network.\n",
    "\n",
    "To take advantage of this, we need to be able to easily define a custom layer from a given function. For instance, PyTorch doesn’t have a view layer, and we need to create one for our network. `LambdaLayer` will create a layer that we can then use when defining a network with `Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, f):\n",
    "        super().__init__()\n",
    "        self._f = f\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self._f(X)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = nn.Sequential(\n",
    "    LambdaLayer(lambda X: X.view(-1, 1, 28, 28)),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    LambdaLayer(lambda X: X.view(X.size(0), -1))\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model_fn.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn,\n",
    "    loss_fn,\n",
    "    training_data_loader,\n",
    "    opt,\n",
    "    validation_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalize our pipeline by wrapping our DataLoader\n",
    "\n",
    "Our CNN is fairly concise, but it only works with MNIST, because:\n",
    "\n",
    "1. It assumes the input is a 28*28 long vector\n",
    "2. It assumes that the final CNN grid size is 4*4 (since that’s the average pooling kernel size we used)\n",
    "\n",
    "Let’s get rid of these two assumptions, so our model works with any 2d single channel image. First, we can remove the initial Lambda layer by moving the data preprocessing into a generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    \n",
    "    def __init__(self, data_loader, f):\n",
    "        self._data_loader = data_loader\n",
    "        self._f = f\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._data_loader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in iter(self._data_loader):\n",
    "            yield self._f(*batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can replace `nn.AvgPool2d` with [torch.nn.AdaptiveAvgPool2d](https://pytorch.org/docs/stable/nn.html#adaptiveavgpool2d), which allows us to define the size of the output tensor we want, rather than the input tensor we have. As a result, our model will work with any size input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    LambdaLayer(lambda X: X.view(X.size(0), -1))\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model_fn.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "_preprocess = lambda X, y: (X.view(-1, 1, 28, 28), y)\n",
    "training_data_loader = WrappedDataLoader(training_data_loader, _preprocess)\n",
    "validation_data_loader = WrappedDataLoader(validation_data_loader, _preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn,\n",
    "    loss_fn,\n",
    "    training_data_loader,\n",
    "    opt,\n",
    "    validation_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPU(s) to accelerate training\n",
    "\n",
    "GPUs can significantly speedup training of deep neural networks. If you are running this notebook in Google Colab, then you can take advantage of free GPUs to accelerate training of your models! To take advantage of GPU acceleration from the tool bar select `Runtime` -> `Change Runtime Type` and then select `GPU` from the hardware accelerator dropdown menu. **Changing the runtime type requires that the Python kernel be restarted and will require you to re-run relevant cells in the notebook!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your deep learning model and make it available to the GPU\n",
    "model_fn = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    LambdaLayer(lambda X: X.view(X.size(0), -1))\n",
    ")\n",
    "model_fn.to(device)\n",
    "\n",
    "# define a loss\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "opt = optim.SGD(model_fn.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "_preprocess = lambda X, y: (X.view(-1, 1, 28, 28).to(device), y.to(device))\n",
    "training_data_loader = WrappedDataLoader(training_data_loader, _preprocess)\n",
    "validation_data_loader = WrappedDataLoader(validation_data_loader, _preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn,\n",
    "    loss_fn,\n",
    "    training_data_loader,\n",
    "    opt,\n",
    "    validation_data_loader,\n",
    "    number_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your own model!\n",
    "\n",
    "Using the above code as a template, try and create your own deep learning model to classify the MNIST data. Here are a few ideas to try.\n",
    "\n",
    "1. Add more convolutional layers.\n",
    "2. Add more neurons in each convolutional layer(s).\n",
    "3. Try different activation layers.\n",
    "4. Try using a different optimizer.\n",
    "5. Try tuning the hyper-parameters of your chosen optimizer.\n",
    "6. Train the model for more epochs (but don't overfit!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = ???\n",
    "model_fn.to(device)\n",
    "\n",
    "opt = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn,\n",
    "    loss_fn,\n",
    "    training_data_loader,\n",
    "    opt,\n",
    "    validation_data_loader,\n",
    "    number_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting to Kaggle\n",
    "\n",
    "If you have created a Kaggle account, then you can submit your model's predictions to Kaggle and see how you stack up against your peers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-train the model using the entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-scale the training features\n",
    "_training_target, _training_features = mnist_arr[:, 0], mnist_arr[:, 1:]\n",
    "_scaled_training_features = min_max_scaler.fit_transform(_training_features)\n",
    "\n",
    "# create the tensors\n",
    "_scaled_training_features_tensor = torch.tensor(_scaled_training_features, dtype=torch.float32)\n",
    "_training_target_tensor = torch.tensor(_training_target)\n",
    "\n",
    "# create the data loader\n",
    "_training_data = data.TensorDataset(_scaled_training_features_tensor, _training_target_tensor)\n",
    "_training_data_loader = data.DataLoader(_training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# wrap the data loader to reshape the data as needed\n",
    "reshape = lambda X, y: (X.view(-1, 1, 28, 28).to(device), y.to(device))\n",
    "wrapped_training_data_loader = WrappedDataLoader(_training_data_loader, reshape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn,\n",
    "    loss_fn,\n",
    "    wrapped_training_data_loader,\n",
    "    opt,\n",
    "    number_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use trained model to make predictions using the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note we use transform method and NOT fit_transform!\n",
    "_testing_features = np.loadtxt(\"../data/raw/mnist/test.csv\", delimiter=',', skiprows=1, dtype=np.int64)\n",
    "_scaled_testing_features = min_max_scaler.transform(_testing_features)\n",
    "scaled_testing_features_tensor = torch.tensor(_scaled_testing_features, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_fn(scaled_testing_features_tensor.view(-1, 1, 28, 28).to(device))\n",
    "predictions = torch.argmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission format for kaggle\n",
    "!head ../data/raw/mnist/sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if not os.path.isdir(\"../data/kaggle-submissions/mnist/\"):\n",
    "    os.makedirs(\"../data/kaggle-submissions/mnist/\")\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "number_predictions, = predictions.shape\n",
    "df = pd.DataFrame({\"ImageId\": range(1, number_predictions + 1), \"Label\": predictions.cpu()})\n",
    "df.to_csv(f\"../data/kaggle-submissions/mnist/submission-{timestamp}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to Kaggle!\n",
    "\n",
    "Once you have successfully submited your predictions then you can check the [Digit-Recognizer competition](https://www.kaggle.com/c/digit-recognizer) website and see how well your best model compares to your peers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export KAGGLE_USERNAME=\"YOUR_USERNAME\"\n",
    "export KAGGLE_KEY=\"YOUR_API_KEY\"\n",
    "kaggle competitions submit digit-recognizer \\\n",
    "  -f $(ls ../data/kaggle-submissions/mnist/submission-*.csv | tail -n 1) \\\n",
    "  -m \"My first ever Kaggle submission!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
