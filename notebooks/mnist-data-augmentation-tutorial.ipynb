{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Performance with Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_arr = np.loadtxt(\"../data/raw/mnist/train.csv\", delimiter=',', skiprows=1, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prng = np.random.RandomState(42)\n",
    "training_features, validation_features, training_target, validation_target = (\n",
    "    model_selection.train_test_split(mnist_arr[:, 1:],\n",
    "                                     mnist_arr[:, 0],\n",
    "                                     test_size=0.10,\n",
    "                                     random_state=_prng)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom `DataSet` class to handle transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(data.Dataset):\n",
    "    \n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, transforms=None):\n",
    "        super().__init__()\n",
    "        self._X = X\n",
    "        self._y = (torch.from_numpy(y)\n",
    "                        .long())  # PyTorch really likes long integers for target dtype!\n",
    "        self._transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        features = self._X[index]\n",
    "        target = self._y[index]\n",
    "        return (features, target) if self._transforms is None else (self._transforms(features), target)\n",
    "    \n",
    "    def __len__(self):\n",
    "        n_samples, _ = self._X.shape\n",
    "        return n_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the DataLoader instances for training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation should only apply to training data\n",
    "_training_transforms = transforms.Compose([\n",
    "    transforms.Lambda(lambda X: X.reshape((28, 28, 1))),\n",
    "    transforms.ToPILImage(mode='L'),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), shear=15, scale=(1.0, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "training_dataset = CustomDataSet(training_features, training_target, _training_transforms)\n",
    "\n",
    "_batch_size = 32\n",
    "training_data_loader = data.DataLoader(training_dataset, batch_size=_batch_size, shuffle=True)\n",
    "\n",
    "# data augmentation should not be applied to validation data\n",
    "_validation_transforms = transforms.Compose([\n",
    "    transforms.Lambda(lambda X: X.reshape((28, 28, 1))),\n",
    "    transforms.ToPILImage(mode='L'),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "validation_dataset = CustomDataSet(validation_features, validation_target, _validation_transforms)\n",
    "validation_data_loader = data.DataLoader(validation_dataset, batch_size=1024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring transformed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , axes = plt.subplots(5, 6, sharex=True, sharey=True, figsize=(20, 20))\n",
    "for i  in range(5):\n",
    "    for j in range(6):\n",
    "        if j == 0:\n",
    "            _ = axes[i, j].imshow(training_features[i].reshape((28, 28)), cmap=\"gray\")\n",
    "        else:\n",
    "            _ = axes[i, j].imshow(training_dataset[i][0][0], cmap=\"gray\")\n",
    "        \n",
    "        if i == 0 and j == 0:\n",
    "            axes[i, j].set_title(\"Original Digit\")\n",
    "        if i == 0 and j > 0:\n",
    "            axes[i, j].set_title(f\"Augmented Digit {j}\")\n",
    "            \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_fit(model_fn, loss_fn, X_batch, y_batch, opt):\n",
    "    # forward pass\n",
    "    loss = loss_fn(model_fn(X_batch), y_batch)\n",
    "\n",
    "    # back propagation\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad() # don't forget to reset the gradient after each batch!\n",
    "\n",
    "\n",
    "def fit(model_fn, loss_fn, training_data_loader, opt, validation_data_loader=None, number_epochs=1):\n",
    "    \n",
    "    for epoch in range(number_epochs):\n",
    "        model_fn.train()\n",
    "        for X_batch, y_batch in training_data_loader:\n",
    "            partial_fit(model_fn, loss_fn, X_batch, y_batch, opt)\n",
    "        \n",
    "        # compute validation loss after each training epoch\n",
    "        if validation_data_loader is not None:\n",
    "            model_fn.eval()\n",
    "            with torch.no_grad():\n",
    "                batch_losses, batch_sizes = zip(*[(loss_fn(model_fn(X), y), len(X)) for X, y in validation_data_loader])\n",
    "                validation_loss = np.sum(np.multiply(batch_losses, batch_sizes)) / np.sum(batch_sizes)\n",
    "            print(f\"Training epoch: {epoch}, Validation loss: {validation_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, f):\n",
    "        super().__init__()\n",
    "        self._f = f\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self._f(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet5 = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    LambdaLayer(lambda X: X.view(X.size(0), -1)),\n",
    "    nn.Linear(256, 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(84, 10)\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "opt = optim.Adam(lenet5.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(lenet5, loss_fn, training_data_loader, opt, validation_data_loader, number_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use our trained model to make predictions using the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_testing_features = np.loadtxt(\"../data/raw/mnist/test.csv\", delimiter=',', skiprows=1, dtype=np.int64)\n",
    "_scaled_testing_features = np.divide(_testing_features, 255, dtype=np.float32)\n",
    "scaled_testing_features_tensor = torch.from_numpy(_scaled_testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = lenet5(scaled_testing_features_tensor.view(-1, 1, 28, 28))\n",
    "predictions = torch.argmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually check model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , axes = plt.subplots(5, 5, sharex=True, sharey=True, figsize=(20, 20))\n",
    "idx = 0\n",
    "for i  in range(5):\n",
    "    for j in range(5):\n",
    "        _ = axes[i, j].imshow(scaled_testing_features_tensor[idx].reshape((28, 28)), cmap=\"gray\")\n",
    "        axes[i, j].set_title(f\"Predicted digit: {predictions[idx]}\")\n",
    "        idx += 1\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission format for kaggle\n",
    "!head ../data/raw/mnist/sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if not os.path.isdir(\"../data/kaggle-submissions/mnist/\"):\n",
    "    os.makedirs(\"../data/kaggle-submissions/mnist/\")\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "number_predictions, = predictions.shape\n",
    "df = pd.DataFrame({\"ImageId\": range(1, number_predictions + 1), \"Label\": predictions.cpu()})\n",
    "df.to_csv(f\"../data/kaggle-submissions/mnist/submission-{timestamp}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to Kaggle!\n",
    "\n",
    "Once you have successfully submited your predictions then you can check the [Digit-Recognizer competition](https://www.kaggle.com/c/digit-recognizer) website and see how well your best model compares to your peers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export KAGGLE_USERNAME=\"YOUR_USERNAME\"\n",
    "export KAGGLE_KEY=\"YOUR_API_KEY\"\n",
    "kaggle competitions submit digit-recognizer \\\n",
    "  -f $(ls ../data/kaggle-submissions/mnist/submission-*.csv | tail -n 1) \\\n",
    "  -m \"My first digit recognizer submission!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
